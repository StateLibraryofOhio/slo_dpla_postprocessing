# slo_dpla_postprocessing
XSLT to run against data exported from REPOX before upload to DPLA


# Purpose

This XSLT file is intended to be run against the XML generated by a harvest of metadata from REPOX as the first step (but not the final step) in post-processing data before we upload it to DPLA.

This XSLT transform will do 2 things:

1.  Remove "deleted" records from the metadata
2.  Insert appropriate metadata for the isReferencedBy element

The isReferencedBy element will be used for transmitting IIIF metadata to DPLA.


# Logic
This XSLT checks the edm:rights metadata associated with the record AND it confirms that the OAI set is intended for IIIF processing.

If the edm:rights is compatible and the OAI set should be processed for IIIF metadata, then an appropriate isReferencedBy metadata value is generated and added to the XML for that record.

If the edm:rights is not compatible, OR if the edm:rights is compatible but the OAI set isn't scheduled to undergo IIIF processing, then an empty isReferencedBy XML tag is added to the record.

# Caveats

The resulting output will not be perfect.  All appropriate records will include properly formatted IIIF metadata, but due to CONTENTdm bugs and/or limitations, some of these URLs may not return valid IIIF metadata from the CONTENTdm server.  The IIIF values should be "confirmed good" before the metadata is forwarded to the DPLA.

One way to confirm the validity of an IIIF URL is to simply load it in a browser.  Does it display properly?

If the Content Management System is CONTENTdm, then a bad IIIF URI will generate a 404 error.  In Islandora (or in Kent's implementation, at least) you are redirected to another page but NOT given a 404.  Because of this, I'm treating CONTENTdm servers and OAKs differently.  

I'm assuming that OAKS URIs are OK.

For CONTENTdm, the fact that we receive a specific 404 error means that we can systematically test these URLs via "wget" and analyze the CONTENTdm server's response, and modify the XML accordingly.

There is, however, a catch.  We have well over 100K CONTENTdm records.  Attempting to check each of these separately is not practical.  (Trust me.  I've tried.)  There is an undocumented workaround that we can use with CONTENTdm to (hopefully) abbreviate the process.

CONTENTdm stores its metadata in a single file on the back-end.  It's XML-like, but does not adhere to XML standards (e.g. special characters are not properly encoded for XML) and thus it cannot be modified slightly and used with an XML parser.  However, in certain cases it is highly grep-able.

Consider the CONTENTdm collection hosted here:  https://ohiomemory.org/digital/collection/p15005coll3

Go to https://ohiomemory.org/robots.txt and note that there are a variety of "Disallow" statements such as:

    Disallow: /digital/collection/p16007coll33/search/

The "/p16007coll33/" section of that statement is a unique identifier for a collection.  We care about the "16007" portion of it.  (Look for a "16###" or a "15***" number when viewing a CONTENTdm robots.txt; these will give you the numbers we want; other collections such as "p267401coll32" include the number, but it's not viable for these purposes.)

Using that 16007, construct a URL similar to:   http://server16007.contentdm.oclc.org/cgi-bin/getdesc.exe?CISOOP=desc&CISOROOT=/p16007coll33

This will download the back-end metadata file (a.k.a. "desc.all").  We will grep through this for clues to IIIF info.

As I mentioned, CONTENTdm metadata files are similar to XML, but different.  Each record is NOT enclosed by a set of other tags; instead, every tag in the file is at the same "level".  The "<title>" tag is always the first field for a record, and the "<dmrecord>" tag is always the last for a record.  Any tags in between are associated.

CONTENTdm records typically have an associated file (image, audio, video, etc.)  The filename for this file is enclosed in "<find>" tags.  The "<dmrecord>" tags enclose the unique ID for the record.  For example, the "<dmrecord>" value for this record would be "371".

  https://ohiomemory.org/digital/collection/p15005coll3/id/371/rec/1

I'm still doing research, but thus far it appears that failures to download IIIF metadata from CONTENTdm have historically occured for 2 reasons.
  
  1.  A CONTENTdm bug.  The filename in the "<find>" tag has a fully-uppercase file extension, and CONTENTdm cannot handle that.  (I've reported this to the CONTENTdm development team and do not know whether it has since been fixed.)
  2.  The file stored in CONTENTdm is not compatible with the CONTENTdm implementation of IIIF.  Some file types are not supported even though the IIIF standard does support them, and ANY file type can be loaded into CONTENTdm (binary executable?  Go for it!)

Downloading the collection's desc.all file allows us to grep through it for '<find>' tags to see whether a collection has problematic file types.  The desc.all at http://server16007.contentdm.oclc.org/cgi-bin/getdesc.exe?CISOOP=desc&CISOROOT=/tlcpl_p16007coll88 has 97,199 records.  Attempting to spider all of them would be...problematic.  So, we can quickly find out the filetypes by downloading the desc.all and:
  
  grep '<find>' desc.all | cut -f 2 -d '.' | cut -f 1 -d '<' | sort | uniq

Output?  One value:  "jp2".   JP2s should be viable if the file extension is lowercase, so we can skip checking these 97K records.  (I hope.)

If we do the same thing for the desc.all at http://server16488.contentdm.oclc.org/cgi-bin/getdesc.exe?CISOOP=desc&CISOROOT=/p16488coll5, then we find that the collection includes file extensions of:   cpd, jp2, jpg, pdfpage

JP2 and JPG files (lowercase) should be OK, but the "cpd" refers to a file outlining a CONTENTdm compound object's structure, and the "pdfpage" is an automatically-generated compound object page that was created when the site imported a PDF using the CONTENTdm Project Client and they specified that the PDF should be displayed as a compound object in the CONTENTdm user interface.  As a result, it might be a good idea to check this collection for problematic IIIF URIs.

Spidering the suspect URLs with "wget" and diverting them into a "valid-URL.txt" or a "not-valid-URL.txt" file allows us to isolate those of a given category.  Once we have the set of bad URLs in a file, we can use "grep -v" to remove the appropriate isReferencedBy values from the appropriate files.

After we replace the "bad" IIIF URIs in the metadata with empty isReferencedBy elements, the metadata is ready to upload to DPLA.

More details/scripts to follow.

.
